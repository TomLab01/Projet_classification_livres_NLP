{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic Modeling avec BERTopic**\n",
        "\n",
        "Auteurs : Tom LABIAUSSE - Pierre Ollivier - Amine CHERIF HAOUAT - Cyrine NABI"
      ],
      "metadata": {
        "id": "Y_Joonm1OWZK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkuts5qoUvXN"
      },
      "source": [
        "# 1. Imports & Connexion GG Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubIW4qw79drm"
      },
      "source": [
        "*   A chaque utilisation du notebook, il faut installer la libraire BERTopic avec la commande : *!pip install BERTopic*.\n",
        "*   Pour ce faire, lancez une fois la cellule ci-dessous. Puis redémarrez le *runtime* comme demandé et relancez la cellule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kysmFc0QeJn"
      },
      "outputs": [],
      "source": [
        "!pip install BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.0.1 # Version compatible avec BERTopic"
      ],
      "metadata": {
        "id": "jDYOz9J7pAFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CZzbUyzB9i1"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5q15j3XJfGT"
      },
      "outputs": [],
      "source": [
        "# Modules classiques\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import cnames as dico_colors\n",
        "COLOR_NAMES = list(dico_colors.keys())\n",
        "import pickle\n",
        "import random\n",
        "import xlwt\n",
        "\n",
        "# Modules pour NLP\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "# from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import scipy.cluster.hierarchy as hcluster\n",
        "import umap\n",
        "import hdbscan\n",
        "\n",
        "# Depuis sklearn\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNHxcNvPTGgs"
      },
      "outputs": [],
      "source": [
        "# Connexion à un Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_to_GGDrive = \"/content/gdrive/MyDrive/\"\n",
        "print(\"> Connexion à Google Drive OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spl1HlEdhCLs"
      },
      "source": [
        "# 2. Chargement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHBJXqQm_UR0"
      },
      "source": [
        "Définition de la fonction *get_dataframe* permettant de charger une base de données Excel dans une dataframe Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmf0XzF57wi6"
      },
      "outputs": [],
      "source": [
        "def get_dataframe(file_name, column_text, column_title=\"Titre\", lower=True, to_spaces=[], min_size=20, max_size=5000, verbose=True):\n",
        "    \"\"\" Charge la base de données depuis un fichier excel dans une dataframe python en effectuant un pré-traitement de base :\n",
        "        - suppression des lignes vides\n",
        "        - remplacement des elements de 'to_spaces' presents dans les resumes par des espace (ex : \"\\n\" ou \"\\t\")\n",
        "        - \"lower=True\" passe tous les resumes en minuscule \"\"\"\n",
        "    input_df = pd.read_excel(file_name)\n",
        "    df = input_df[['EAN', column_title, column_text]]\n",
        "\n",
        "    # Renommage des colonnes (column_title -> titre ; column_text -> texte)\n",
        "    df.columns = ['EAN','titre','texte']\n",
        "\n",
        "    # Suppression des lignes vides\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Ajout d'une colonne avec la taille des textes\n",
        "    df['taille_texte'] = 0 # valeur par defaut\n",
        "\n",
        "    # Suppression des sequences de 'to_spaces' dans les textes\n",
        "    for k in range(0,df.shape[0]):\n",
        "        for elt in to_spaces:\n",
        "            df.iloc[k,2] = df.iloc[k,2].replace(elt,\" \").strip(\" \") # texte\n",
        "            df.iloc[k,1] = df.iloc[k,1].replace(elt,\" \").strip(\" \") # texte\n",
        "        if lower:\n",
        "            df.iloc[k,2] = df.iloc[k,2].lower() # texte\n",
        "            df.iloc[k,1] = df.iloc[k,1].lower() # titre\n",
        "        df.iloc[k,3] = len(df.iloc[k,2])\n",
        "    \n",
        "    # Suppression des textes de taille incoherentes\n",
        "    min_ind = df.loc[df['taille_texte']<min_size].index\n",
        "    df.drop(min_ind, inplace=True)\n",
        "    max_ind = df.loc[df['taille_texte']>max_size].index\n",
        "    df.drop(max_ind, inplace=True)\n",
        "\n",
        "    if verbose:\n",
        "      print('# Dataframe \"{0}\" de taille {1} chargee.'.format(file_name,df.shape))\n",
        "      print(\"> {0} textes de taille <{1} ou >{2} supprimes\\n\".format(len(min_ind)+len(max_ind),min_size,max_size))\n",
        "      df\n",
        "\n",
        "    return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYxKSzeEB-Mi"
      },
      "source": [
        "Définition de la fonction *merge_dataframe* permettant de concatener différentes dataframes Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHyZJ6tyB949"
      },
      "outputs": [],
      "source": [
        "def merge_dataframe(df_list):\n",
        "    \"\"\" Fusion des dataframes de 'df_list' dans la dataframe 'merged_df'. \"\"\"\n",
        "    if len(df_list) == 0:\n",
        "        print(\"Aucune dataframe.\")\n",
        "        merged_df = None\n",
        "    else:\n",
        "        merged_df = df_list[0]\n",
        "        for k in range(1,len(df_list)):\n",
        "            merged_df = merged_df.append(df_list[k], ignore_index=True)\n",
        "        print(\"# Fusion des corpus dans une dataframe de taille {0}\".format(merged_df.shape))\n",
        "    return(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do2XkiYbCoHQ"
      },
      "source": [
        "Chargement des dataframes et construction du dataset fusionné."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pboqqIEYJkO7"
      },
      "outputs": [],
      "source": [
        "corpus_num = [1,2]\n",
        "to_spaces = [\"\\n\",\"\\t\",\"&nbsp;\"]\n",
        "corpus_list = []\n",
        "\n",
        "if 1 in corpus_num: # CORPUS 1\n",
        "    corpus1 = get_dataframe(path_to_GGDrive + 'corpus 1.xlsx', column_text=\"Description sans html\", to_spaces=to_spaces)\n",
        "    corpus_list.append(corpus1)\n",
        "\n",
        "if 2 in corpus_num: # CORPUS 2\n",
        "    corpus2 = get_dataframe(path_to_GGDrive + 'corpus 2.xlsx', column_text=\"Description\", to_spaces=to_spaces)\n",
        "    corpus_list.append(corpus2)\n",
        "\n",
        "if 3 in corpus_num: # CORPUS 3\n",
        "    corpus3 = get_dataframe(path_to_GGDrive + 'corpus 3.xlsx', column_text=\"resume\", to_spaces=to_spaces)\n",
        "    corpus_list.append(corpus3)\n",
        "\n",
        "# Concatenation des corpus\n",
        "data = merge_dataframe(corpus_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRP1IPlb-sk"
      },
      "source": [
        "Définition de la fonction *get_docs* permettant de récupérer les données pour l'entraînement (concaténation éventuelle des titres aux textes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44ky6m6HGYSA"
      },
      "outputs": [],
      "source": [
        "def get_docs(data, add_titles=False):\n",
        "    \"\"\" Récupère les textes et ajoute concatene les titres si besoin. \"\"\"\n",
        "    docs = data[\"texte\"].to_list() ; titles_ = data[\"titre\"].to_list()\n",
        "    if add_titles:\n",
        "        for k in range(0,len(texts_)):\n",
        "            docs[k] = docs[k] + \". \" + titles_[k]\n",
        "    del(titles_)\n",
        "    print(\"> Nombre de textes : \",len(docs))\n",
        "    return(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1Rz_slMoQDe"
      },
      "outputs": [],
      "source": [
        "# DONNEES D'ENTRAINEMENT\n",
        "docs = get_docs(data, add_titles=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbVl2xbvhPzm"
      },
      "source": [
        "# 3. Algorithme BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTNtQHahVrx"
      },
      "source": [
        "## 3.1 Entraîner un modèle BERTopic sur *docs*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjzbX7GIrfZ5"
      },
      "source": [
        "### 3.1.1 Transformer CamemBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNI96lvEtopw"
      },
      "outputs": [],
      "source": [
        "# Installation de la librairie flair\n",
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmjJPw-9JmeE"
      },
      "outputs": [],
      "source": [
        "# Import d'un document-Transformer\n",
        "from flair.embeddings import TransformerDocumentEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T96XcfkHIljw"
      },
      "source": [
        "Initialisation d'un modèle BERT avec CamemBERT\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp1rfQDkIH8E"
      },
      "outputs": [],
      "source": [
        "camembert = TransformerDocumentEmbeddings('camembert-base')\n",
        "BERT_model = BERTopic(embedding_model=camembert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN47ZjrxrjgR"
      },
      "source": [
        "### 3.1.2 Transformer BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ervOJLIv4M"
      },
      "source": [
        "Initialisation d'un modèle BERT avec BERT (from Google)\n",
        "\n",
        "*   language = *french* ou *multilingual*\n",
        "*   verbose = True : affiche les details de l'execution\n",
        "*   nr_topics = *None*, *'auto'* ou un nombre de topics fixé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxLq-SRiqlf9"
      },
      "outputs": [],
      "source": [
        "BERT_model = BERTopic(language=\"multilingual\", verbose=True, nr_topics=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcLalWSzqvOI"
      },
      "source": [
        "### 3.1.3 Application de BERTopic au corpus *docs* (2/3 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSgrXb0kVgqJ"
      },
      "outputs": [],
      "source": [
        "doc2topics, probabilities = BERT_model.fit_transform(docs)\n",
        "doc2topics = np.array(doc2topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP4XbOJDhhZU"
      },
      "source": [
        "## 3.2 Charger/Enregistrer un modèle BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ENggQaJKq4V"
      },
      "source": [
        "Nom du fichier d'enregistrement/sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caDvLGu5iMEd"
      },
      "outputs": [],
      "source": [
        "model_file_name = \"BERTopic_#1+2_t30_04-01-22\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfeasaNiNoOa"
      },
      "source": [
        "Définition des fonctions de sauvegarde/téléchargement des fichiers de correspondances :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6R40uAqlzGu"
      },
      "outputs": [],
      "source": [
        "# DOC -> TOPIC\n",
        "\n",
        "def save_doc2topic(model_file_name,np_topics):\n",
        "    \"\"\" Fonction de sauvegarde des correspondances document-topic dans un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Doc2Topic.txt\",'wb') as fichier:\n",
        "        outil = pickle.Pickler(fichier)\n",
        "        outil.dump(np_topics)\n",
        "    print(\"> Doc2Topic file '{0}' saved in '{1}'\".format(model_file_name+\"_Doc2Topic.txt\",path_to_GGDrive + \"BERT_models/\"))\n",
        "\n",
        "def load_doc2topic(txt_file):\n",
        "    \"\"\" Fonction de telechargement des correspondances document-topic depuis un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Doc2Topic.txt\",'rb') as fichier:\n",
        "        outil = pickle.Unpickler(fichier)\n",
        "        topics = outil.load()\n",
        "    return(topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeqQpneOcyCO"
      },
      "outputs": [],
      "source": [
        "# TOPIC -> BIG TOPIC\n",
        "\n",
        "def save_topic2bigtopic(model_file_name,topic_clustering):\n",
        "    \"\"\" Fonction de sauvegarde des correspondances topic_bigtopic dans un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Topic2Bigtopic.txt\",'wb') as fichier:\n",
        "        outil = pickle.Pickler(fichier)\n",
        "        outil.dump(topic_clustering)\n",
        "    print(\"> Topic2Bigtopic file '{0}' saved in '{1}'\".format(model_file_name+\"_Topic2Bigtopic.txt\",path_to_GGDrive + \"BERT_models/\"))\n",
        "\n",
        "def load_topic2bigtopic(model_file_name):\n",
        "    \"\"\" Fonction de chargement des correspondances topic_bigtopic depuis un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Topic2Bigtopic.txt\",'rb') as fichier:\n",
        "        outil = pickle.Unpickler(fichier)\n",
        "        topic_clustering = outil.load()\n",
        "    return(topic_clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBxsc9RRryxa"
      },
      "outputs": [],
      "source": [
        "# DOC -> BIG TOPCIC\n",
        "\n",
        "def save_doc2bigtopic(model_file_name,doc2bigtopics):\n",
        "    \"\"\" Fonction de sauvegarde des correspondances docs-bigtopics dans un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Doc2Bigtopic.txt\",'wb') as fichier:\n",
        "        outil = pickle.Pickler(fichier)\n",
        "        outil.dump(doc2bigtopics)\n",
        "    print(\"> Topic2Bigtopic file '{0}' saved in '{1}'\".format(model_file_name+\"_Doc2Bigtopic.txt\",path_to_GGDrive + \"BERT_models/\"))\n",
        "\n",
        "def load_doc2bigtopic(model_file_name):\n",
        "    \"\"\" Fonction de chargement des correspondances docs-bigtopics depuis un fichier .txt \"\"\"\n",
        "    with open(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_Doc2Bigtopic.txt\",'rb') as fichier:\n",
        "        outil = pickle.Unpickler(fichier)\n",
        "        doc2bigtopics = outil.load()\n",
        "    return(doc2bigtopics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxEaA2DtiSVd"
      },
      "source": [
        "Enregistrement dans *BERT_model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS95gSE8es2O"
      },
      "outputs": [],
      "source": [
        "# Sauvegarde du modèle BERTopic contenu dans model_file_name\n",
        "BERT_model.save(path_to_GGDrive + \"BERT_models/\" + model_file_name)\n",
        "print(\"> Model '{0}' saved in '{1}'\".format(model_file_name,path_to_GGDrive + \"BERT_models/\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yObeQ6ORpgbK"
      },
      "outputs": [],
      "source": [
        "# Sauvegarde des correspondances document-topic\n",
        "save_doc2topic(model_file_name,doc2topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOpRci83iZSc"
      },
      "source": [
        "Chargement dans *BERT_model*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTMxy0fZfriN"
      },
      "outputs": [],
      "source": [
        "# Chargement du modèle BERTopic contenu dans model_file_name\n",
        "BERT_model = BERTopic.load(path_to_GGDrive + \"BERT_models/\" + model_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK5zZyxkzQud"
      },
      "outputs": [],
      "source": [
        "# Chargement des correspondances document-topic\n",
        "doc2topics = load_doc2topic(model_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0Aq8ZGkyQB4"
      },
      "outputs": [],
      "source": [
        "# Chargement d'éventuelles correspondances topic-bigtopic\n",
        "topic_clustering = load_topic2bigtopic(model_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwsMHzKWyPE2"
      },
      "outputs": [],
      "source": [
        "# Chargement d'éventuelles correspondances doc-bigtopic\n",
        "doc2bigtopics = load_doc2bigtopic(model_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uXFZuR6h17u"
      },
      "source": [
        "# 4. Visualisation des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8AOjBZHsjKu"
      },
      "source": [
        "Aperçu général des topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrK-AJqysjYb"
      },
      "outputs": [],
      "source": [
        "BERT_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXdsiQpGXNmh"
      },
      "source": [
        "Aperçu des résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6Ns1pxnyodG"
      },
      "outputs": [],
      "source": [
        "nb_words = 10\n",
        "print(\"#=================================================== TOPICS ===================================================#\")\n",
        "for ind in range(0,BERT_model.get_topic_info().shape[0]):\n",
        "    print( \"TOPIC {0} -> \".format(ind-1) + \" \".join([word for [word,_] in BERT_model.get_topic(ind-1)][:nb_words]) )\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Répartition générale des documents dans les topics"
      ],
      "metadata": {
        "id": "4PUkjyroPftA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sizes(n_clusters, doc2clusters, show_outliers=False):\n",
        "    \"\"\" Affiche le nombre de texte de chaque cluster (cluster = topic ou big topic) dans la repartition doc2cluster (doc2cluster = doc2topics ou doc2bigtopics) \"\"\"\n",
        "    c_sizes = [ [c,len(np.where(doc2clusters == c)[0])] for c in range(-1,n_clusters) ]\n",
        "    c_sizes = np.array(sorted(c_sizes, key=lambda x : x[1], reverse = True))\n",
        "    if show_outliers:\n",
        "        plt.plot(range(-1,n_clusters),c_sizes[:,1])\n",
        "    else:\n",
        "        plt.plot(range(0,n_clusters),c_sizes[1:,1])\n",
        "    plt.title('Répartition de {0} textes dans les {1} clusters'.format(len(doc2clusters)-c_sizes[0,1],n_clusters))\n",
        "    plt.xlabel(\"Clusters\") ; plt.ylabel(\"Nombre de textes\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4PrcFXiOPeAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sizes(BERT_model.get_topic_info().shape[0]-1, doc2topics)"
      ],
      "metadata": {
        "id": "iEyTIR-IPex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97NlsH4psIdV"
      },
      "source": [
        "Affichage des documents appartenant à un topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0FVUNl5Vkut"
      },
      "outputs": [],
      "source": [
        "def print_docs_in_topic(t):\n",
        "    \"\"\" Affiche les documents appartenant au topic 't.' \"\"\"\n",
        "    docs_per_topic = [ list(np.where(doc2topics==k)[0]) for k in range(-1,np.max(doc2topics)+1)]\n",
        "    print( \"# TOPIC {0} : \".format(t) +  \" \".join([elt[0] for elt in BERT_model.get_topic(t)]) ) ; print(\"\")\n",
        "    for doc_id in docs_per_topic[t+1]:\n",
        "        print(\"[doc ID : {0}] - {1}\".format(doc_id,docs[doc_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN6kZCQNYyt8"
      },
      "outputs": [],
      "source": [
        "print_docs_in_topic(96)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pj8ZpWRsDrs"
      },
      "source": [
        "Intertopic Distance Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbfBDdCSbyYE"
      },
      "outputs": [],
      "source": [
        "BERT_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lR_WkECI9c"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder l'Intertopic Distance Map interactive\n",
        "BERT_model.visualize_topics().write_html(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_IDM.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLXFMyuUWZgy"
      },
      "source": [
        "Identification de topic par mot-clef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQTG55EjWZgz"
      },
      "outputs": [],
      "source": [
        "word = \"crayon\"\n",
        "\n",
        "close_topics, similarity = BERT_model.find_topics(word, top_n=5)\n",
        "for t,s in zip(close_topics,similarity):\n",
        "    print(\"- {0}% = TOPIC {1} : \".format(int(100*s),t) +  \" \".join([elt[0] for elt in BERT_model.get_topic(t)]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUhGaZTkWZg0"
      },
      "source": [
        "Classification d'un nouveau texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTix_AobWZg1"
      },
      "outputs": [],
      "source": [
        "txt_test = \"\"\"\n",
        "L'ère de l'intelligence artificielle est déjà là ! Les robots vont tous nous remplacer parce que les ordinateurs sont plus rapides que les humains.\n",
        "\"\"\"\n",
        "\n",
        "[topic_test],proba_test = BERT_model.transform(txt_test)\n",
        "print(\"TEXTE : \" + txt_test)\n",
        "print( \"PREDICTION : {0}% - TOPIC {1} : \".format(proba_test,topic_test) +  \" \".join([elt[0] for elt in BERT_model.get_topic(topic_test)]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OaVuX2xixUc"
      },
      "source": [
        "# 5. Evaluation des résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLa1mVtNEvBI"
      },
      "source": [
        "Le code suivant ecrit par Maarten Grootendorst est disponible à l'adresse : https://github.com/MaartenGr/BERTopic/issues/90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8OycAdVuMpj"
      },
      "outputs": [],
      "source": [
        "def compute_coherence(docs,topics):\n",
        "    \"\"\" Calcul la mesure de coherence CV sur un modèle BERTopic. \"\"\"\n",
        "    # Preprocess Documents\n",
        "    documents = pd.DataFrame({\"Document\": docs,\n",
        "                              \"ID\": range(len(docs)),\n",
        "                              \"Topic\": topics})\n",
        "    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "    cleaned_docs = BERT_model._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "    # Extract vectorizer and analyzer from BERTopic\n",
        "    vectorizer = BERT_model.vectorizer_model\n",
        "    analyzer = vectorizer.build_analyzer()\n",
        "\n",
        "    # Extract features for Topic Coherence evaluation\n",
        "    words = vectorizer.get_feature_names()\n",
        "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
        "    dictionary = corpora.Dictionary(tokens)\n",
        "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
        "    topic_words = [[words for words, _ in BERT_model.get_topic(topic)] \n",
        "                  for topic in range(len(set(topics))-1)]\n",
        "\n",
        "    # Evaluate\n",
        "    coherence_model = CoherenceModel(topics=topic_words, \n",
        "                                    texts=tokens, \n",
        "                                    corpus=corpus,\n",
        "                                    dictionary=dictionary, \n",
        "                                    coherence='c_v')\n",
        "    \n",
        "    return(coherence_model.get_coherence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ0aZXs4Hfk8"
      },
      "outputs": [],
      "source": [
        "coherence = compute_coherence(docs,doc2topics)\n",
        "print(\"Coherence C_v : \",coherence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2LV_qgbSUqC"
      },
      "source": [
        "# 6. Réduction du nombre de topics (avec les *topic embeddings*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu8aZNczGtg"
      },
      "source": [
        "## 6.1 Réalisation de la réduction sur un modèle BERTopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBTeZeLp5-qU"
      },
      "source": [
        "*   La méthode suivante s'inspire du code permettant de construire l'intertopic-map disponible à l'adresse : https://github.com/MaartenGr/BERTopic/blob/master/bertopic/plotting/_topics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJQZzmSKTd5D"
      },
      "source": [
        "6.1.1 - Projection des topics en 2D avec UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDbo--4J-P6e"
      },
      "outputs": [],
      "source": [
        "def get_topic_embeddings(bert_model,n_umap=2):\n",
        "    \"\"\" Renvoie les topic embeddings obtenus avec 'bert_model' apres reduction en 'n-umap' dimensions. \"\"\"\n",
        "    n_topics = len(list(BERT_model.get_topics().keys()))\n",
        "    topic_emb = bert_model.c_tf_idf.toarray()[np.arange(0,n_topics)]\n",
        "    topic_emb = MinMaxScaler().fit_transform(topic_emb)\n",
        "    topic_emb = umap.UMAP(n_neighbors=2, n_components=n_umap, metric='hellinger').fit_transform(topic_emb)\n",
        "    return(topic_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLxb3pkP_n7t"
      },
      "outputs": [],
      "source": [
        "topic_emb = get_topic_embeddings(BERT_model)[1:] # On ne s'interesse pas au topic -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUIz1ntvTaIH"
      },
      "source": [
        "6.1.2 Réalisation d'un clustering hiérarchique sur les topics (choix du *threshold* par optimisation du score de silhouette)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IkVgl6BNZrw"
      },
      "outputs": [],
      "source": [
        "# Calcul des coef de silhouette pour plusieurs threshold\n",
        "many_thld = np.arange(0.2,4.0,step=0.1).round(2)\n",
        "s_scores =[]\n",
        "for thld in many_thld:\n",
        "    t_clustering = hcluster.fclusterdata(topic_emb, thld, criterion=\"distance\")-1\n",
        "    s_scores.append(silhouette_score(topic_emb, t_clustering))\n",
        "\n",
        "# Affichage de l'evolution du coef de silhouette avec le threshold\n",
        "s_scores = np.array(s_scores)\n",
        "plt.plot(many_thld,s_scores)\n",
        "plt.title(\"Evolution du score de silhouette du clustering de topcis\")\n",
        "plt.show()\n",
        "\n",
        "# Meilleur(s) threshold(s)\n",
        "best_score = np.max(s_scores)\n",
        "best_ind = list(np.where(s_scores==best_score)[0])\n",
        "best_thlds = [many_thld[i] for i in best_ind]\n",
        "print(\"Best silhouette_score = {0} with threshold in {1}\".format(best_score,best_thlds))\n",
        "\n",
        "# Choix d'un threshold et clustering final\n",
        "best_thld = best_thlds[len(best_thlds)//2]\n",
        "print(\"> Choosen threshold : {0}\".format(best_thld))\n",
        "topic_clustering = hcluster.fclusterdata(topic_emb, best_thld, criterion=\"distance\")-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svmyab4JTQUZ"
      },
      "source": [
        "6.1.3 Affichage du clustering sur la projection des topics en 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppHH47UJB7n4"
      },
      "outputs": [],
      "source": [
        "def plot_topics(top_emb, clustering=[], dcolor=\"blue\"):\n",
        "    \"\"\" Affiche une representation des topics projetes en 2D avec visualisation des clusters par couleurs si 'clustering' precise. \"\"\"\n",
        "    n_topics = top_emb.shape[0]\n",
        "    x_top_emb = topic_emb[:,0].reshape(-1)\n",
        "    y_top_emb = topic_emb[:,1].reshape(-1)\n",
        "    if len(clustering) == 0: # No clustering\n",
        "        clustering = np.zeros(n_topics)\n",
        "        top_colors = [dcolor for k in range(n_topics)]\n",
        "        plt.scatter(x_top_emb, y_top_emb, c=top_colors, marker=\"^\")\n",
        "        plt.title(\"Topic embeddings en 2D\") ; plt.show()\n",
        "    else:\n",
        "        n_clusters = len(np.unique(clustering))\n",
        "        colors = random.sample(COLOR_NAMES,n_clusters)\n",
        "        top_colors = [colors[int(clust)] for clust in clustering]\n",
        "        plt.scatter(x_top_emb, y_top_emb, c=top_colors, marker=\"^\")\n",
        "        plt.title(\"Topic embeddings en 2D [#{0}>>#{1}]\".format(n_topics,n_clusters)) ; plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5zxQKVs0s9V"
      },
      "outputs": [],
      "source": [
        "plot_topics(topic_emb,topic_clustering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llC42U4eqSAb"
      },
      "source": [
        "6.1.4 - Association de chaque texte a un big topic. On définit le big topic -1 comme étant exactement le topic -1 (big topic des outliers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms_2X83KncaP"
      },
      "outputs": [],
      "source": [
        "doc2bigtopics = np.copy(doc2topics)\n",
        "for k in range(0,len(doc2topics)):\n",
        "    if doc2topics[k] == -1: # outlier\n",
        "        doc2bigtopics[k] = -1\n",
        "    else:\n",
        "      doc2bigtopics[k] = topic_clustering[doc2topics[k]]\n",
        "print(doc2bigtopics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRkBdss3rZ_s"
      },
      "source": [
        "6.1.5 - Sauvegarde des correspondances [textes - big topics] et [topic - big topic]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6MToRoAwxOq"
      },
      "outputs": [],
      "source": [
        "save_topic2bigtopic(model_file_name,topic_clustering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrdInp1zdJG_"
      },
      "outputs": [],
      "source": [
        "save_doc2bigtopic(model_file_name,doc2bigtopics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtJguhtlroWS"
      },
      "source": [
        "## 6.2 - Visualisation des big topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SidnV7EzlPwR"
      },
      "outputs": [],
      "source": [
        "# Topics triés par BigTopic\n",
        "big_topics = [list(np.where(topic_clustering==k)[0]) for k in range(0,np.max(topic_clustering)+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxtj1xTK0V75"
      },
      "source": [
        "6.2.1 Visualisation des textes par big topic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_sizes(len(big_topics),doc2bigtopics) # Répartition générale des textes par big topic ('get_sizes' definie en section 4. du notebook)"
      ],
      "metadata": {
        "id": "vLHT6ko-LO4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsfkIZabrGLU"
      },
      "outputs": [],
      "source": [
        "bt_num = 16\n",
        "nb_txt = 200\n",
        "\n",
        "print(\"> BIG TOPIC {0}\".format(bt_num))\n",
        "k = 0 ; i = 0\n",
        "while (k<len(doc2bigtopics)) and (i<nb_txt):\n",
        "    if doc2bigtopics[k] == bt_num:\n",
        "        i+=1 ; print(\"#{0} : {1}\".format(i,docs[k][:150]))\n",
        "    k+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f3yx0mwTJxN"
      },
      "source": [
        "6.2.2 - Visualisation des topics composant les big topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeMxUPQkFKtB"
      },
      "outputs": [],
      "source": [
        "nb_words = 10\n",
        "print(\"#=================================================== TOPICS of BIG TOPICS ===================================================#\\n\")\n",
        "for bt in range(0,len(big_topics)):\n",
        "    btopic = big_topics[bt]\n",
        "    print(\"# BIG TOPIC {0} = {1}\".format(bt,btopic))\n",
        "    for t in btopic:\n",
        "        print( \"TOPIC {0} -> \".format(t) + \" \".join([word for [word,_] in BERT_model.get_topic(t)][:nb_words]) )\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfaBuPhTaXaZ"
      },
      "source": [
        "6.2.3 - Homogénéisation des big topics (par matrice TF ou TF-iDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5ykqlekQV5"
      },
      "outputs": [],
      "source": [
        "# Liste des indices de textes appartenant à chaque big topic\n",
        "docs_per_bigtopic = [ list(np.where(doc2bigtopics==k)[0]) for k in range(0,np.max(doc2bigtopics)+1)]\n",
        "\n",
        "# Formation d'un super-texte à partir des textes composant chaque big topic\n",
        "txt_of_bigtopic = np.array([ \" \".join( [docs[doc_ind] for doc_ind in docs_per_bigtopic[bt_ind]] ) for bt_ind in range(0,len(big_topics)) ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei2k5gIngUAp"
      },
      "outputs": [],
      "source": [
        "# Telechargement d'une liste de stop words français\n",
        "f = open(path_to_GGDrive + 'stop_words_french.txt') ; fr_stopwords = set(f.read().split('\\n')) ; f.close()\n",
        "print(\"NB french stopwords :\",len(fr_stopwords))\n",
        "\n",
        "# Méthode de classement des mots TF (Term Frequency)\n",
        "def c_TF(documents,my_stop_words):\n",
        "    \"\"\" count_matrix = tf \"\"\"\n",
        "    count = CountVectorizer(stop_words=my_stop_words).fit(documents)\n",
        "    t = count.transform(documents).toarray() # num_clusters x num_words\n",
        "    w = t.sum(axis=1) # num_clusters x 1\n",
        "    count_matrix = np.divide(t.T, w) # num_words x num_clusters\n",
        "    return(count_matrix, count)\n",
        "\n",
        "# Méthode de classement des mots TF_iDF (Term Frequency - inverse Document Frequency)\n",
        "def c_TF_IDF(documents,my_stop_words):\n",
        "    \"\"\" count_matrix = tf-idf \"\"\"\n",
        "    count = CountVectorizer(stop_words=my_stop_words).fit(documents)\n",
        "    t = count.transform(documents).toarray() # num_clusters x num_words\n",
        "    w = t.sum(axis=1) # num_clusters x 1\n",
        "    tf = np.divide(t.T, w) # num_words x num_clusters\n",
        "    D = len(documents) # num_clusters x 1\n",
        "    sum_t = t.sum(axis=0) # num_words x 1\n",
        "    idf = np.log(np.divide(D, sum_t)).reshape(-1, 1) # num_words x 1\n",
        "    count_matrix = np.multiply(tf, idf) # num_words x num_clusters\n",
        "    return(count_matrix, count)\n",
        "\n",
        "def extract_top_n_words_per_topic(tf_idf, count, n_labels, n=10, inf_size=3):\n",
        "    \"\"\" Fonction d'extraction des 'n' mots les plus representatifs d'un texte en ne considerant que les mots de plus de 'inf_size' caracteres. \"\"\"\n",
        "    words = count.get_feature_names()\n",
        "    labels = np.arange(0,n_labels)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    for l in top_n_words.values():\n",
        "        s = []\n",
        "        for k in range(0,len(l)):\n",
        "            if len(l[k][0])<inf_size:\n",
        "                s.append(k)\n",
        "        if len(s)>0:\n",
        "            s.reverse()\n",
        "            for k in s:\n",
        "                l.pop(k)\n",
        "    return(top_n_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR0RhVG5gXG8"
      },
      "outputs": [],
      "source": [
        "count_matrix, count = c_TF(txt_of_bigtopic, my_stop_words=fr_stopwords)\n",
        "top_n_words = extract_top_n_words_per_topic(count_matrix, count, n_labels=len(big_topics), n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hFBXFfUrSS2"
      },
      "outputs": [],
      "source": [
        "nb_words = 10\n",
        "print(\"#=================================================== WORDS of BIG TOPICS ===================================================#\")\n",
        "for bt in range(0,len(big_topics)):\n",
        "    btopic = big_topics[bt]\n",
        "    # print(\"# BIG TOPIC {0} = {1}\".format(bt,btopic))\n",
        "    print( \" BIG TOPIC {0} -> \".format(bt) + \" \".join([word for [word,_] in top_n_words[bt]][:min(nb_words,len(top_n_words[bt]))]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loxrZtLldBUP"
      },
      "source": [
        "# 7. Sauvegarde du Topic Modeling (EXCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o2R8A9ldbeN"
      },
      "source": [
        "Nécessite au minimum les deux éléments suivants :\n",
        "*   Dataframe des données à l'origine de l'entraînement du modèle : *data*\n",
        "*   Liste de correspondance docs-topics (*doc2topics*) ou docs-bigtopics (*doc2bigtopics*)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bia3R0gNeZOW"
      },
      "outputs": [],
      "source": [
        "# Données d'entraînement du modèle\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrQZFyjPgMAI"
      },
      "outputs": [],
      "source": [
        "# Mots descriptifs de chaque topic\n",
        "nb_words = 10\n",
        "topic2words = [\" \".join([word for [word,_] in BERT_model.get_topic(t)][:nb_words]) for t in range(0,max(doc2topics)+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrL-puj5iFah"
      },
      "outputs": [],
      "source": [
        "# Mots descriptifs de chaque big topic (necessite de realiser l'homogeneisation des big topics en amont -> section 6.2.3 du notebook)\n",
        "nb_words = 10\n",
        "bigtopic2words = [\" \".join([word for [word,_] in top_n_words[bt]][:min(nb_words,len(top_n_words[bt]))]) for bt in range(0,len(top_n_words))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wojNfSZpk_eh"
      },
      "source": [
        "Définition de la fonction *save_docs_topics* permettant de sauvegarder dans un fichier .xls les correspondances docs-topics ou docs_bigtopics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKqCiNQddIiT"
      },
      "outputs": [],
      "source": [
        "def save_docs_topcis(topic_type, docs2numtopics, top2words):\n",
        "    if topic_type in [\"topic\",\"bigtopic\"]:\n",
        "        # Creation du fichier excel\n",
        "        tab = xlwt.Workbook()\n",
        "\n",
        "        # Feuille 1\n",
        "        sheet1 = tab.add_sheet('topic_modeling')\n",
        "        sheet1.write(0, 0, 'EAN') ; sheet1.write(0, 1, 'TITRE') ; sheet1.write(0, 2, 'TEXTE') ; sheet1.write(0, 3, 'TOPIC')\n",
        "        for k in range(0,data.shape[0]):\n",
        "            sheet1.write(k+1, 0, int(data.iloc[k,0])) # EAN\n",
        "            sheet1.write(k+1, 1, data.iloc[k,1]) # TITRE\n",
        "            sheet1.write(k+1, 2, data.iloc[k,2]) # TEXTE\n",
        "            sheet1.write(k+1, 3, int(docs2numtopics[k])) # TOPIC\n",
        "\n",
        "        # Feuille 2\n",
        "        sheet2 = tab.add_sheet('topic_visualisation')\n",
        "        sheet2.write(0, 0, 'TOPIC') ; sheet2.write(0, 1, 'DESCRIPTION')\n",
        "        for k in range(0,len(top2words)):\n",
        "            sheet2.write(k+1, 0, k) # TOPIC\n",
        "            sheet2.write(k+1, 1, top2words[k]) # DESCRIPTION\n",
        "        \n",
        "        tab.save(path_to_GGDrive + \"BERT_models/\" + model_file_name + \"_DATA_TOPICS.xls\")\n",
        "    else:\n",
        "        print(\"ERREUR > topic_type doit être 'topic' ou 'bigtopic'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3wbtDEIlaVC"
      },
      "outputs": [],
      "source": [
        "save_docs_topcis(\"bigtopic\", doc2bigtopics, bigtopic2words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l6Q8myCQT6X"
      },
      "source": [
        "# 8. Réduction du nombre de topics (avec la méthode BERTopic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR6nE3i6jTTX"
      },
      "source": [
        "*   **ATTENTION !** : Réduire le nombre de topic modifie de manière irréversible le modèle entraîné. Impossible de revenir aux anciens topics une fois la réduction effectuée.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPL417E2ygL"
      },
      "source": [
        "8.1 Réduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uon7a1bmQUFi"
      },
      "outputs": [],
      "source": [
        "# REDUCTION\n",
        "new_doc2topic_, new_probabilities = BERT_model.reduce_topics(docs, doc2topics, nr_topics=24) # probabilities is optional\n",
        "new_doc2topic = np.array(new_doc2topic_)\n",
        "del(new_doc2topic_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_doc2topic, _ = BERT_model.reduce_topics(docs, doc2topics, nr_topics=30)"
      ],
      "metadata": {
        "id": "SwDHdiCbS-O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFG3K0aeaBch"
      },
      "outputs": [],
      "source": [
        "BERT_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaGGLKc-Z6s3"
      },
      "outputs": [],
      "source": [
        "# Apercu des topics\n",
        "nb_words = 10\n",
        "print(\"#=================================================== TOPICS ===================================================#\")\n",
        "for ind in range(0,BERT_model.get_topic_info().shape[0]):\n",
        "    print( \"TOPIC {0} -> \".format(ind-1) + \" \".join([word for [word,_] in BERT_model.get_topic(ind-1)][:nb_words]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0haR1QCtHp6"
      },
      "outputs": [],
      "source": [
        "# Intertopic distance map\n",
        "BERT_model.visualize_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOT-W93A2UuS"
      },
      "source": [
        "8.2 Visualisation TF des mots par topics (nécessite les fonctions *c_TF* et *extract_top_n_words_per_topic* de 6.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w2i_39sfnUY"
      },
      "outputs": [],
      "source": [
        "new_doc2topic = doc2topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cu4-Wz9eXiq"
      },
      "outputs": [],
      "source": [
        "# Liste des indices de textes appartenant à chaque big topic\n",
        "docs_per_newtopic = [ list(np.where(new_doc2topic==k)[0]) for k in range(0,np.max(new_doc2topic)+1)]\n",
        "\n",
        "# Formation d'un super-texte pour chaque big topic\n",
        "txt_per_newtopic = np.array([ \" \".join( [docs[doc_ind] for doc_ind in docs_per_newtopic[bt_ind]] ) for bt_ind in range(0,len(docs_per_newtopic)) ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmpsSHCZeXit"
      },
      "outputs": [],
      "source": [
        "count_matrix, count = c_TF(txt_per_newtopic, my_stop_words=fr_stopwords)\n",
        "top_n_words = extract_top_n_words_per_topic(count_matrix, count, n_labels=len(docs_per_newtopic), n=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_9n0iDIeXit"
      },
      "outputs": [],
      "source": [
        "nb_words = 10\n",
        "print(\"#=================================================== NEW TOPICS ===================================================#\")\n",
        "for nt in range(0,len(docs_per_newtopic)):\n",
        "    print( \"NEW TOPIC {0}\".format(nt) + \" -> \" + \" \".join([word for [word,_] in top_n_words[nt]][:min(nb_words,len(top_n_words[nt]))]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYOp_8iB2Rs1"
      },
      "source": [
        "8.3 Visualisation des documents par topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z6RyyWnaf2d"
      },
      "outputs": [],
      "source": [
        "# Aperçu des documents par topic\n",
        "def print_new_docs_in_topic(t):\n",
        "    \"\"\" Affiche les documents appartenant au topic 't.' \"\"\"\n",
        "    new_docs_per_topic = [ list(np.where(new_doc2topic==k)[0]) for k in range(-1,np.max(new_doc2topic)+1)]\n",
        "    print( \"# TOPIC {0} : \".format(t) +  \" \".join([elt[0] for elt in BERT_model.get_topic(t)]) ) ; print(\"\")\n",
        "    for doc_id in new_docs_per_topic[t+1]:\n",
        "        print(\"[doc ID : {0}] - {1}\".format(doc_id,docs[doc_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G1-p2neagb1"
      },
      "outputs": [],
      "source": [
        "print_new_docs_in_topic(17)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xvCiqRQ3Ca-"
      },
      "source": [
        "8.4 Calcul du score de cohérence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03xlm_HZ_9B"
      },
      "outputs": [],
      "source": [
        "# Evaluation du topic modeling\n",
        "coherence = compute_coherence(docs,new_doc2topic)\n",
        "print(\"Coherence C_v : \",coherence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUubTT0TittX"
      },
      "source": [
        "8.5 Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VormyAbCcnfN"
      },
      "outputs": [],
      "source": [
        "model_file_name = \"BERTopic_#3_t24_13-01-22\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxISMRcrckgt"
      },
      "outputs": [],
      "source": [
        "BERT_model.save(path_to_GGDrive + \"BERT_models/\" + model_file_name)\n",
        "print(\"> Model '{0}' saved in '{1}'\".format(model_file_name,path_to_GGDrive + \"BERT_models/\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox92YWVhckgu"
      },
      "outputs": [],
      "source": [
        "save_doc2topic(model_file_name,new_doc2topic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xa8KN3QBZeC"
      },
      "source": [
        "# 9. Algorithme BERT + UMAP + HDBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDyTpgbqX2qg"
      },
      "source": [
        "## 9.1 Exécution des trois algorithmes successifs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwiytuFhT6Il"
      },
      "outputs": [],
      "source": [
        "# Creation des document-embeddings\n",
        "my_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "embeddings = my_model.encode(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGJoekGcUVdP"
      },
      "outputs": [],
      "source": [
        "# Reduction de la dimensions des projections des documents avec UMAP\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5,\n",
        "                            min_dist=0.0,\n",
        "                            metric='cosine').fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnXenqyWZy5Z"
      },
      "outputs": [],
      "source": [
        "# Realisation du clustering sur les projections d'UMAP avec HDBSCAN\n",
        "clustering = hdbscan.HDBSCAN(min_cluster_size=50, \n",
        "                          metric='euclidean', \n",
        "                          cluster_selection_method='eom'\n",
        "                          ).fit(umap_embeddings)\n",
        "print(f\"Number of clusters = {np.unique(clustering.labels_).shape[0] - 1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSXETZInXb5S"
      },
      "source": [
        "## 9.2 Aperçu des topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCPOcaDiYR7I"
      },
      "source": [
        "Taille des topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHQ7o7X_EM9G"
      },
      "outputs": [],
      "source": [
        "# Correspondances document-topic\n",
        "my_doc2topic = clustering.labels_\n",
        "\n",
        "# Nombre de documents par topic\n",
        "topic_size = np.bincount(my_doc2topic+1)\n",
        "topic_ind = np.arange(-1,len(topic_size)-1)\n",
        "pd.DataFrame(data = np.vstack((topic_ind,topic_size)).T, columns=['topic','size']).sort_values(by='size',ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dYfPFXnYUtd"
      },
      "source": [
        "Mots représentatifs de chaque topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia-wNhBjWOEj"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SytkVlIGtpP"
      },
      "outputs": [],
      "source": [
        "docs_df = pd.DataFrame(docs, columns=['document'])\n",
        "docs_df['topic'] = my_doc2topic\n",
        "docs_per_topic = docs_df.groupby(['topic']).agg({'document': ' '.join})\n",
        "docs_per_topic = docs_per_topic.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH3mvhThewSk"
      },
      "outputs": [],
      "source": [
        "def CTfIDF(documents):\n",
        "    count = CountVectorizer(stop_words=fr_stop).fit(documents)\n",
        "    t = count.transform(documents).toarray() #num_clusters x num_words\n",
        "    w = t.sum(axis=1) # num_clusters x 1\n",
        "    tf = np.divide(t.T, w) #num_words x num_clusters\n",
        "    D = len(documents) # num_clusters x 1\n",
        "    sum_t = t.sum(axis=0) #num_words x 1\n",
        "    idf = np.log(np.divide(D, sum_t)).reshape(-1, 1) # num_words x 1\n",
        "    tf_idf = np.multiply(tf, idf) #num_words x num_clusters\n",
        "    return(tf_idf, count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joN9IYhkKaBR"
      },
      "outputs": [],
      "source": [
        "tf_idf, count = CTfIDF(docs_per_topic['document'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuPayH_WK_S1"
      },
      "outputs": [],
      "source": [
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names()\n",
        "    labels = list(docs_per_topic.topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return(top_n_words)\n",
        "\n",
        "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9Qsv8m0XOst"
      },
      "outputs": [],
      "source": [
        "top_n_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27WrpntjXb5d"
      },
      "outputs": [],
      "source": [
        "nb_words = 10\n",
        "print(\"#=================================================== TOPICS ===================================================#\")\n",
        "for ind in range(0,len(topic_size)):\n",
        "    print( \"TOPIC {0} -> \".format(ind-1) + \" \".join([word for [word,_] in top_n_words[ind-1][:nb_words]]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VF6B1c4Xb5e"
      },
      "source": [
        "## 9.3 Affichage des documents appartenant à un topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCmaqoCrTMhG"
      },
      "outputs": [],
      "source": [
        "docs_per_topic = [ list(np.where(my_doc2topic==k)[0]) for k in range(-1,np.max(my_doc2topic)+1)]\n",
        "\n",
        "def print_docs_in_topic_(t):\n",
        "    \"\"\" Affiche les documents appartenant au topic 't.' \"\"\"\n",
        "    print( \"TOPIC {0} : \".format(t) + \" \".join([word for [word,_] in top_n_words[t][:nb_words]]) ) ; print(\"\")\n",
        "    for doc_id in docs_per_topic[t+1]:\n",
        "        print(\"[doc ID : {0}] - {1}\".format(doc_id,docs[doc_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LysOS7SgTYvu"
      },
      "outputs": [],
      "source": [
        "print_docs_in_topic_(14)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tkuts5qoUvXN",
        "spl1HlEdhCLs",
        "0gTNtQHahVrx",
        "DjzbX7GIrfZ5",
        "KN47ZjrxrjgR",
        "ZcLalWSzqvOI",
        "RP4XbOJDhhZU",
        "5uXFZuR6h17u",
        "4OaVuX2xixUc",
        "Bxu8aZNczGtg",
        "BtJguhtlroWS",
        "loxrZtLldBUP",
        "2l6Q8myCQT6X",
        "-xa8KN3QBZeC",
        "LDyTpgbqX2qg",
        "RSXETZInXb5S",
        "-VF6B1c4Xb5e"
      ],
      "name": "Topic_Modeling_BERTopic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}