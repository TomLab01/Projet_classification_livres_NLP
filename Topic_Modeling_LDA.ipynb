{"cells":[{"cell_type":"markdown","metadata":{"id":"Y_Joonm1OWZK"},"source":["# **Topic Modeling avec LDA**\n","\n","Auteurs : Tom LABIAUSSE - Pierre Ollivier - Amine CHERIF HAOUAT - Cyrine NABI"]},{"cell_type":"markdown","metadata":{"id":"xIr7Tr6VeT36"},"source":["# 1# Imports\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EQxx1yKBxDS"},"outputs":[],"source":["# Imports des bibliothèques classiques\n","import sys\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","from tqdm import tqdm_notebook\n","tqdm_notebook().pandas()\n","from tqdm import tqdm\n","\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","nltk.download('punkt')\n","\n","# \"lemmatizer\" = FRENCH Tokenizer AND Lemmatizer (from spacy)\n","!python -m spacy download fr_core_news_md\n","import fr_core_news_md\n","lemmatizer = fr_core_news_md.load()\n","\n","# Imports nécessaires pour LDA\n","import gensim\n","from gensim import models\n","from gensim.models import Phrases, CoherenceModel\n","from gensim import corpora\n","\n","from nltk.tokenize import word_tokenize\n","\n","# Imports pour l'optimisation Bayesienne\n","from bayes_opt import BayesianOptimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NR8Qvm8Unfko"},"outputs":[],"source":["print(gensim.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yv4htM1DrK2I"},"outputs":[],"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n"," \n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZMH6qpmrMei"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","print(\"Connexion à Google Drive OK\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HGWIXC5rQwe"},"outputs":[],"source":["path_to_GGDrive = \"/content/gdrive/MyDrive/\""]},{"cell_type":"markdown","metadata":{"id":"mJrDyYjCnjzX"},"source":["# 2# Chargement des données"]},{"cell_type":"markdown","metadata":{"id":"ks7V_jlYaZXC"},"source":["### 2.1# Chargement des corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmf0XzF57wi6"},"outputs":[],"source":["def get_dataframe(file_name, column_texte, lower=False, to_spaces=[]):\n","    \"\"\" Charge la base de données dans une dataframe python en effectuant un pré-traitement de base :\n","        - suppression des lignes vides\n","        - suppression des sauts de lignes et indentations dans les resumes \n","        to_spaces permet de préciser quels autres séquences doivent être remplacées par des espaces dans le texte.\"\"\"\n","    input_df = pd.read_excel(file_name)\n","    df = input_df[['EAN',\"Titre\",column_texte]]\n","    df.columns = ['EAN','Titre','Texte']\n","    # Suppression des lignes vides\n","    df = df.dropna().reset_index(drop=True)\n","    # Suppression des sauts de lignes et indentations dans les resumes\n","    for k in range(0,df.shape[0]):\n","        for elt in to_spaces:\n","            df.iloc[k,1] = df.iloc[k,1].replace(elt,\" \")\n","        if lower:\n","            df.iloc[k,1] = df.iloc[k,1].lower()\n","    print('Dataframe with shape {1} loaded from \"{0}\"\\n'.format(file_name,df.shape))\n","    return(df)"]},{"cell_type":"markdown","metadata":{"id":"FUCA3_Pja3HO"},"source":["Si vous travaillez sur Colab, n'oubliez pas d'importer les fichiers corpus 1, corpus 2, stop_words_french et stop_symbols en cliquant sur la petite icône dossier à gauche.\n","\n","Sinon, il faudra changer les chemin d'accès aux documents dans ce notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpraK8Bynjzc"},"outputs":[],"source":["corpus1 = get_dataframe(path_to_GGDrive+'data_corpus/corpus 1.xlsx', column_texte = 'Description sans html', lower=True, to_spaces=[\"\\n\",\"\\t\",\"&nbsp;\"])\n","corpus2 = get_dataframe(path_to_GGDrive+'data_corpus/corpus 2.xlsx', column_texte = 'Description', lower=True, to_spaces=[\"\\n\",\"\\t\",\"&nbsp;\"])\n","corpus3 = get_dataframe(path_to_GGDrive+'data_corpus/corpus_editis.xlsx', column_texte = 'resume', lower=True, to_spaces=[\"\\n\",\"\\t\",\"&nbsp;\"])\n","\n","newcorpus = corpus1\n","corpus1and2 = newcorpus.append(corpus2, ignore_index=True)"]},{"cell_type":"markdown","metadata":{"id":"szepC8tlZI_U"},"source":["### 2.2# Choix du corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IanL8PyZSy6"},"outputs":[],"source":["# Choisir entre corpus1, corpus2, corpus1and2 et corpus3\n","data = corpus1and2"]},{"cell_type":"markdown","metadata":{"id":"kFSNDybvgQLQ"},"source":["# 3# Chargement des stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Jz3hNGpghC5"},"outputs":[],"source":["# stopwords spéciaux propres au corpus\n","special_stopwords = set(['',\"\",' ',\" \",'  ',\"  \",'nbsp','pa','faire','al','grâce','homme','monde','grand', '\\n','ouvrage','livre','nouveau','politique','pouvoir','histoire'])\n","\n","# Mots récurrents pouvant être ajoutés au choix dans les stopwords ci-dessus :\n","# 'politique','pouvoir','histoire'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAxIBmBNEc07"},"outputs":[],"source":["# Chargement des stopwords francais & stopsymbols\n","f = open(path_to_GGDrive+'data_corpus/stop_words_french.txt') ; fr_stopwords = set(f.read().split('\\n')) ; f.close() ; print(\"NB french stopwords : \",len(fr_stopwords))\n","f = open(path_to_GGDrive+'data_corpus/stop_symbols.txt') ; stopsymbols = set(f.read().split('\\n')) ; f.close() ; print(\"NB stopsymbols : \",len(stopsymbols))\n","print(\"Nombre de 'special' stopwords : \",len(special_stopwords))\n","my_stopwords = fr_stopwords.union(stopsymbols,special_stopwords)\n","print(\"Nombre total de stopwords : \",len(my_stopwords))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUPChh8EbbAk"},"outputs":[],"source":["def hist_length(elements):\n","    stop_by_length = {}\n","    for e in elements:\n","        if len(e) in stop_by_length:\n","            stop_by_length[len(e)].append(e)\n","        else:\n","            stop_by_length[len(e)] = [e]\n","    sorted_tuples = sorted([(t,len(u)) for (t,u) in list(stop_by_length.items())], key = lambda x : x[0])\n","    plt.plot([x[0] for x in sorted_tuples],[x[1] for x in sorted_tuples])\n","    plt.title('Histogramme des tailles des éléments de \"my_stopwords\"')\n","    plt.xlabel(\"Taille\") ; plt.ylabel(\"Effectif\")\n","    return(stop_by_length)\n","stopwords_dico = hist_length(my_stopwords)"]},{"cell_type":"markdown","metadata":{"id":"kbHbbeyNhKjw"},"source":["On n'observe pas de pic très étroit autour d'une taille typique. Il peut donc être intéréssant d'ordonner les stopwords par taille pour pouvoir les retrouver plus rapidement. On range les stopwords par taille dans le dictionnaire \"stopwords_dico\" dont les clefs sont les tailles."]},{"cell_type":"markdown","metadata":{"id":"t4vhhg6Anjzf"},"source":["# 4# Tokenization & Lemmatization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0cgY_fR0upy"},"outputs":[],"source":["# Exemple d'utilisation de \"lemmatizer\" A DECOMMENTER POUR FAIRE UN TEST\n","\"\"\"\n","txt = \"Mangerai\"\n","tokens_lemma = lemmatizer(txt) # dirons ! dirait\n","for token in tokens_lemma:\n","    print([token], \"->\", [token.lemma_])\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNPKW9T2ATk9"},"outputs":[],"source":["# Définition une borne inf acceptable pour la taille des mots\n","taille_inf = 2\n","\n","# Creation de la colonne des tokens lemmatisés\n","data['tokens'] = None\n","\n","# Lemmatizer les resumes du corpus\n","len_stopwords = list(stopwords_dico.keys())\n","for k in tqdm(range(0,data.shape[0]), position=0, leave=True):\n","    txt = data['Titre'][k] + ' ' + data['Texte'][k]\n","    txt_tokens = []\n","\n","    tokens_lemma = lemmatizer(str(txt))\n","    for token in tokens_lemma:\n","        lemma_token = str(token.lemma_).strip(\" \") # Permet d'enlever les espaces superflus\n","        u = len(lemma_token)\n","        if (u >= taille_inf) and (not(u in len_stopwords) or not(lemma_token in stopwords_dico[u])):\n","            txt_tokens.append(lemma_token)\n","    data['tokens'][k] = txt_tokens"]},{"cell_type":"markdown","metadata":{"id":"lIy1SCQFnjzl"},"source":["# 5# LDA"]},{"cell_type":"markdown","metadata":{"id":"LaOEsfnEnjzm"},"source":["### 5.1# Data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zODyYUYnjzm"},"outputs":[],"source":["tokens = data['tokens'].tolist()"]},{"cell_type":"markdown","metadata":{"id":"4cqygsd_w2nr"},"source":["> Préparation des modèles bi-grams et tri-grams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_jpKcgZLjrU"},"outputs":[],"source":["## Décommenter la ligne bigram_model et la ligne tokens pour créer des groupes de 2 mots max\n","## Décommenter la ligne bigram_model, trigram_model et la ligne tokens pour créer des groupes de 3 mots max\n","\n","# bigram_model = Phrases(tokens)\n","# trigram_model = Phrases(bigram_model[tokens], min_count=1)\n","# tokens = list(trigram_model[bigram_model[tokens]])"]},{"cell_type":"markdown","metadata":{"id":"HRipzTKhnjzm"},"source":["> Préparation des objets utiles à LDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HroABzL4njzn"},"outputs":[],"source":["dictionary_LDA = corpora.Dictionary(tokens)\n","dictionary_LDA.filter_extremes(no_below=3)\n","corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"]},{"cell_type":"markdown","metadata":{"id":"jTuQbNv7njzn"},"source":["### 5.2.1# Choix 1 : **Exécution** de LDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SizIBC3fNMZ"},"outputs":[],"source":["# Choix des hyperparamètres\n","\n","num_topics = 30\n","alpha = 0.59\n","eta = 0.59\n","passes = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x5pA467OzXJ6"},"outputs":[],"source":["lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","                               id2word=dictionary_LDA,\n","                               num_topics=num_topics,\n","                               chunksize=100,\n","                               passes=passes,\n","                               alpha=alpha,\n","                               eta=eta,\n","                               random_state = 42)\n","\n","coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary_LDA, coherence='c_v')\n","print('Coherence Score: ',coherence_model_lda.get_coherence())"]},{"cell_type":"markdown","metadata":{"id":"bqeezu-0y2KP"},"source":["Pour sauvegarder le modèle :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqb24cGOy1LJ"},"outputs":[],"source":["lda_model.save('lda_model.model')"]},{"cell_type":"markdown","metadata":{"id":"wejoTz0EwKub"},"source":["### 5.2.2# Choix 2 : **Chargement d'un modèle** de LDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpqNobvGwQ5g"},"outputs":[],"source":["lda_model = gensim.models.LdaModel.load('lda_model.model')"]},{"cell_type":"markdown","metadata":{"id":"zOxs1oxpnjzn"},"source":["### 5.3# Aperçu des topics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3JNXJOsnjzo","scrolled":true},"outputs":[],"source":["# Aperçu pondéré des topics\n","\n","for i,topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=True):\n","    print(str(i)+\": \"+ topic)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4xQ-WGJc0EkF"},"outputs":[],"source":["# Aperçu non pondéré des topics\n","\n","lda_topics = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n","for tp in lda_topics:\n","    tp_words = [wd[0] for wd in tp[1]]\n","    print(\"TOPIC {0} :\".format(tp[0]),\" \".join(tp_words))"]},{"cell_type":"markdown","metadata":{"id":"3G6BgInLnjzo"},"source":["### 5.4# Allocation de topics aux documents et création d'un fichier csv contenant les résultats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yVrlyc0uK2n"},"outputs":[],"source":["# Ajouter à la dataframe le topic de chaque document\n","\n","data['Topic'] = None\n","\n","for k in tqdm(range(data.shape[0]), position=0, leave=True):\n","\n","  probas = []\n","  indexes = []\n","  prediction_k = lda_model[corpus[k]].copy()\n","  length = len(prediction_k)\n","\n","  if length == 0:\n","    data['Topic'][k] = 'Sans topic'\n","  \n","  else:\n","    for j in range(length):\n","      indexes.append(prediction_k[j][0])\n","      probas.append(prediction_k[j][1])\n","    data['Topic'][k] = indexes[np.argmax(probas)]"]},{"cell_type":"markdown","metadata":{"id":"5UWRSSSWcRMI"},"source":["Pour créer un fichier .csv contenant le topic de chaque document, exécuter la cellule suivante. Si vous travaillez sur coloab, le fichier se trouvera dans le même dossier que celui où vous avez importé les documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhVY7t2w-wVT"},"outputs":[],"source":["# Pour créer un fichier csv contenant le topic de chaque document, exécuter cette cellule\n","pd.DataFrame(data[['EAN','Titre','Topic']]).to_csv('document_topic_allocation.csv', sep=';', index=False)"]},{"cell_type":"markdown","metadata":{"id":"FfxIvgl9njzo"},"source":["### 5.5# Prédiction de topics d'un nouveau document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgifPaHdcjxc"},"outputs":[],"source":["# Insérer l'exemple ici. Attention aux apostrophes et aux guillements!\n","document = \"Cyril Lignac cuisine chez lui pour toi ! Envie d'une cuisine maison savoureuse et rapide ? En direct de sa cuisine, Cyril Lignac te propose 45 recettes salées et/ou sucrées pour mettre un peu de peps dans ton quotidien. Un risotto aux coquillettes, un poisson au four à l'huile vierge et aux petits légumes ou encore une fabuleuse tarte aux fraises ou des petits pots de crème à la vanille... Tu vas te régaler en toute simplicité ! Un livre indispensable, ultra-pratique et sans prétention, pour égayer tes déjeuners et dîners ; des recettes gourmandes, croquantes, craquantes, à déguster en solo, à deux, en famille ou entre amis. Avec Cyril, le fait-maison c'est ultra-facile ! Mets ton tablier et laisse-toi guider par ses précieux conseils et ses recettes ultra-réconfortantes.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mrYFvDXnjzo"},"outputs":[],"source":["token_exeample = word_tokenize(document)\n","topics = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n","pd.DataFrame([(el[0], round(el[1],2), topics[el[0]][1]) for el in lda_model[dictionary_LDA.doc2bow(token_exeample)]], columns=['topic #', 'weight', 'words in topic'])"]},{"cell_type":"markdown","metadata":{"id":"4n_rpKzcNn3v"},"source":["# 6# Optimisation des paramètres\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hlYexY4MlKdU"},"source":["### 6.1# Préparation des outils d'optimisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WiZ92aBNn4D"},"outputs":[],"source":["def print_inventory(dct):\n","    print(\"Items held:\")\n","    for item, amount in dct.items():  # dct.iteritems() in Python 2\n","        print(\"{} ({})\".format(item, amount))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2ItxPoCOrWp"},"outputs":[],"source":["# Fonction support\n","def compute_coherence_values(corpus, dictionary, k, a, b):\n","    \n","    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n","                               id2word=dictionary_LDA,\n","                               num_topics=num_topics,\n","                               chunksize=100,\n","                               passes=5,\n","                               alpha=a,\n","                               eta=b,\n","                               random_state = 42)\n","    \n","    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary_LDA, coherence='c_v')\n","    \n","    return coherence_model_lda.get_coherence()"]},{"cell_type":"markdown","metadata":{"id":"sVlZ4xydlnz5"},"source":["### 6.2# Grid search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Z7dw7-sOswG"},"outputs":[],"source":["grid = {}\n","grid['Validation_Set'] = {}\n","\n","# Range du nombre de topics\n","min_topics = 4\n","max_topics = 7\n","step_size = 1\n","topics_range = range(min_topics, max_topics+1, step_size)\n","\n","# Paramètre alpha\n","alpha = list(np.arange(0.1, 1.01, 0.1))\n","\n","# Paramètre bêta\n","beta = list(np.arange(0.1, 1.01, 0.1))\n","\n","model_results_grid = {'Topics': [],\n","                 'Alpha': [],\n","                 'Beta': [],\n","                 'Coherence': []\n","                }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BimZQOzoGvXC"},"outputs":[],"source":["# Prend beaucoup de temps à tourner\n","\n","pbar = tqdm(total=(len(beta)*len(alpha)*len(topics_range)))\n","    \n","# iterate through number of topics\n","for k in topics_range:\n","    # iterate through alpha values\n","    for a in alpha:\n","        # iterare through beta values\n","        for b in beta:\n","            # get the coherence score for the given parameters\n","            cv = compute_coherence_values(corpus=corpus, dictionary=dictionary_LDA, \n","                                                  k=k, a=a, b=b)\n","            # Save the model results\n","            model_results_grid['Topics'].append(k)\n","            model_results_grid['Alpha'].append(a)\n","            model_results_grid['Beta'].append(b)\n","            model_results_grid['Coherence'].append(cv)\n","                    \n","            pbar.update(1)\n","pd.DataFrame(model_results_grid).to_csv('lda_tuning_results_grid_search.csv', sep=',', index=False)\n","pbar.close()"]},{"cell_type":"markdown","metadata":{"id":"X1OuSMzhlqnn"},"source":["### 6.3# Random search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F70nBs6ynE2I"},"outputs":[],"source":["model_results_random = {'Topics': [],\n","                 'Alpha': [],\n","                 'Beta': [],\n","                 'Coherence': []\n","                }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSvUyT7odVNe"},"outputs":[],"source":["# Nombre d'itérations\n","iter = 10\n","\n","# limite inférieure du nombre de topics\n","nb_topic_inf = 7\n","\n","# limite supérieure du nombre de topics\n","nb_topic_sup = 11\n","\n","# Can take a long time to run\n","\n","for j in tqdm(range(iter), position=0, leave=True):\n","  nb_topics = np.random.randint(nb_topic_inf,nb_topic_sup)\n","  alpha = np.random.random()\n","  beta = np.random.random()\n","\n","  cv = compute_coherence_values(corpus=corpus, dictionary=dictionary_LDA, \n","                                                  k=nb_topics, a=alpha, b=beta)\n","  # Save the model results\n","  model_results_random['Topics'].append(nb_topics)\n","  model_results_random['Alpha'].append(alpha)\n","  model_results_random['Beta'].append(beta)\n","  model_results_random['Coherence'].append(cv)\n","\n","pd.DataFrame(model_results_random).to_csv('lda_tuning_results_random.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"uEVMeQE7P1Fa"},"source":["### 6.4# Optimisation bayésienne"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqKcyrE7RDvz"},"outputs":[],"source":["# Choix de la région de l'espace des paramètres\n","pbounds = {'a': (0.01, 0.7), 'b': (0.01, 0.7)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kpb2YLPDRGm-"},"outputs":[],"source":["import tqdm\n","pbar = tqdm.tqdm(10)\n","\n","liste=[]\n","max=0\n","a=0\n","b=0\n","Kopt=0\n","\n","for k in range(6,13): #on fait une optimisation bayésienne pour chaque nombre de topic k dans une plage choisie\n","  \n","  def black_box_function(a, b): \n","    return compute_coherence_values(corpus, dictionary_LDA, k, a, b)\n","\n","  optimizer = BayesianOptimization(\n","    f=black_box_function,\n","    pbounds=pbounds,\n","    verbose=2, \n","    random_state=1,\n","  )\n"," \n","  optimizer.maximize(\n","      init_points=0,  # choix du nombre d'initialisation lors d'une optimisation\n","      n_iter=5,  # choix du nombre d'itération pour chaque optimisation \n","  )\n","  liste.append(k)\n","  liste.append(optimizer.max)\n","  if max< optimizer.max['target']:\n","    max=optimizer.max['target']\n","    parametres=optimizer.max['params']\n","    a=parametres['a']\n","    b=parametres['b']\n","    Kopt=k\n","  pbar.update(1)\n","pbar.close()\n","print(liste)\n","\n","\n","print(\"Le nombre de topic optimal est \", Kopt)\n","print(\"Alpha optimal est  \", a)\n","print(\"Beta optimal est  \", b)\n","print(\"Pour un score de cohérence de \")\n","print(optimizer.max['target'])"]},{"cell_type":"markdown","metadata":{"id":"u8fMwl7dnjzp"},"source":["# 7# Exploration avancée des topics de LDA"]},{"cell_type":"markdown","metadata":{"id":"JWZJn7Q1njzp"},"source":["#### Allocation des topics dans tous les documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt4TbSounjzp"},"outputs":[],"source":["topics = [lda_model[corpus[i]] for i in range(len(data))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P0aeALOznjzp"},"outputs":[],"source":["# Définition de la fonction d'allocation des poids de chaque topic à un document donné\n","\n","def topics_document_to_dataframe(topics_document, num_topics):\n","    res = pd.DataFrame(columns=range(num_topics))\n","    for topic_weight in topics_document:\n","        res.loc[0, topic_weight[0]] = topic_weight[1]\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kUXiAQDpnjzp"},"outputs":[],"source":["# Les lignes correspondent aux documents, les colonnes aux topics, et la case (i,j) correspondant au poids du topic j dans le document i\n","document_topic = \\\n","pd.concat([topics_document_to_dataframe(topics_document, num_topics=num_topics) for topics_document in topics]) \\\n","  .reset_index(drop=True).fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HO7uSgeTnjzp","scrolled":true},"outputs":[],"source":["document_topic.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nlK9nJvnjzq","scrolled":false},"outputs":[],"source":["# Exemple : affichage par ordre décroissant de poids des documents associés au topic 5\n","topic = 0\n","\n","document_topic.sort_values(topic, ascending=False)[topic].head(10)"]},{"cell_type":"markdown","metadata":{"id":"8UJXS8dunjzq"},"source":["#### Observation de la distribution des topics dans tous les documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZxogBglnjzq"},"outputs":[],"source":["%matplotlib inline\n","import seaborn as sns; sns.set(rc={'figure.figsize':(30,50)})\n","sns.heatmap(document_topic.loc[document_topic.idxmax(axis=1).sort_values().index])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wMIOoGWnjzq"},"outputs":[],"source":["# Nombre de documents appartenant à chaque topic\n","\n","sns.set(rc={'figure.figsize':(10,5)})\n","document_topic.idxmax(axis=1).value_counts().plot.bar(color='lightblue')"]}],"metadata":{"colab":{"collapsed_sections":["xIr7Tr6VeT36","mJrDyYjCnjzX","kFSNDybvgQLQ","t4vhhg6Anjzf","lIy1SCQFnjzl","wejoTz0EwKub","4n_rpKzcNn3v","u8fMwl7dnjzp"],"name":"Topic_Modeling_LDA.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}